# Wikipedia Dataset Analysis


## Summary

* [Goal](#goal)
* [Project Development](#project-development)
* [Development Environment](#environment)
* [Dataset](#dataset)
* [Project Structure](#structure)
* [Insights](#insights)
* [Feature Engineering](#engineering)
	- [Features used in First Revert Scenario](#engineering-first-revert)
	- [Features used in Any Revert Scenario](#engineering-any-revert)
* [Predicting if user will edit again after first revert](#first-revert-model)
* [Predicting if user will edit again after any revert](#any-revert-model)

<h2 id='goal'>Goal</h2>

The goal this analysis is to obtain insights on the data of the Wiki Challange and to 
predict if a user is likely to contribute again after an update was reverted. 

Obs: This is **not** the same goal as the original challenge on Kaggle.

<h2 id='project-development'>Project Development</h2>

The project was started by making some exploratory analysis on the datasets described on the section [Dataset](#dataset). All insights can be seen on the [Insights](#insights) section. 

The first idea was to construct a simpler model, that would check the probability of a user edit again after the first 
edit was reverted. This is a simpler model because it would have only 1 record for each user, considering its first revert only. This approach will be called `First revert scenario` and these models results can be seen on the section [Predicting if user will edit again after first revert](#first-revert-model).

After a good model was achieved considering just the first revert, a model was built considering the probability of the user edit again after **any** revert. In this case, the dataset was much bigger, since one user could have multiple edits reverted. To make this dataset smaller for optimization purpose, a sample of 500,000 reverted edits were considered. This approach will be called `Any revert scenario` and these models results can be seen on the section [Predicting if user will edit again after any revert](#any-revert-model).

All feature engineering used by both models are described in the section [Feature Engineering](#engineering).


<h2 id='environment'>Development Environment</h2>

This analysis was made using Python 3.7.

Create an virtualenv using either virtualenv or conda. Example using conda:

`$ conda create -n wiki python=3.7`

Then install the requirements using *pip*:

`$ pip install -r requirements.txt`


<h2 id='dataset'>Dataset</h2>

The data is available [on this kaggle page](https://www.kaggle.com/c/wikichallenge).
Dataset was divided into 5 separated files.

* Categories
* Comments
* Edits
* Namespaces
* Titles

For this project, only the file `edits.tsv` was used for the purpose of this analysis. The file `comments.tsv` were used for a brief insight analysis, to see the possibility of adding comments as a feature.

The dataset `edits.tsv` constituted of 22,126,031 records of edition and 11 columns of information. Not all columns had metadata about them, neither on Kaggle website nor on other pages available from the webpage. 
The list of information and its metadata that were available can be observed below:

| Name                 | Detail                                  | Type of Data |
|----------------------|-----------------------------------------|--------------|
| user_id              | Number of the user that did the editing | Integer      |
| article_id           | Identifier of the article edited        | Integer      |
| revision_id          | Identifier of the number of revision    | Integer      |
| namespace            | -                                       | Integer      |
| timestamp            | Moment when the article was edited      | Datetime     |
| md5                  | -                                       | Integer      |
| reverted             | Flag if the edit was reverted           | Bool         |
| reverted_user_id     | -                                       | Integer      |
| reverted_revision_id | -                                       | Integer      |
| delta                | -                                       | Integer      |
| cur_size             | -                                       | Integer      |


The dataset `comments.tsv` consisted in 18,978,698 records with 2 columns of information: the revision id and the comment made.  



<h2 id='structure'>Project Structure</h2>

This project was divided in three main folders: 

* **data**: all available data
	- **external**: data gather from external fonts
	- **processed**: all data generated by our processes
* **notebook**: all notebooks used on this project
	- **insights**: all notebooks used for exploratory analysis
	- **modelling**: all notebooks used for training and testing models
	- **provision**: all notebooks used for feature engineering and data manipulation
* **models**: the files of all models trained.

However, the data folder will not be included to this repository due to the large 
amount of data. The processed data can be obtained with the notebooks inside the 
`provision` folder, and the external data can be found on the 
[kaggle website](https://www.kaggle.com/c/wikichallenge).


```
|-- README.md
|
|-- data
|   |-- external
|   |   |-- categories.tsv
|   |   |-- comments.tsv
|   |   |-- edits.tsv
|   |   |-- namespaces.tsv
|   |   `-- titles.tsv
|   `-- processed
|       |-- reverts_sampled_500k.csv
|       `-- user_first_revert.csv
|   
|-- models
|   |-- any_revert_model_v1.pkl
|   `-- first_revert_model_v1.pkl
|
|-- notebooks
|   |
|   |-- insights
|   |   |-- 01-Insights-Edits.ipynb
|   |   `-- 02-Insights-Comments.ipynb
|   |
|   |-- modelling
|   |   |-- any_revert
|   |   |   |-- 1.01-Baseline_LogisticRegression.ipynb
|   |   |   |-- 1.02-LogisticRegression_undersampling.ipynb
|   |   |   |-- 1.03-RandomForest_undersampling_GridSearchCV.ipynb
|   |   |   `-- tools.py
|   |   |-- first_revert
|   |   |   |-- 01-Baseline_LogisticRegression.ipynb
|   |   |   |-- 02-LogisticRegression-undersampling.ipynb
|   |   |   |-- 03-RandomForest.ipynb
|   |   |   |-- 04-RandomForest-undersampling.ipynb
|   |   |   |-- 05-GradientBoosting.ipynb
|   |   |   |-- 06-GradientBoosting-undersampling.ipynb
|   |   |   `-- tools.py
|   |
|   `-- provision
|       |-- 00-Edits-Feature-Engineering-First-Revert.ipynb
|       `-- 01-Edits-Feature-Engineering-Any-Revert.ipynb
|   
`-- requirements.txt
```

<h2 id='insights'>Insights</h2>

### Edits

All analysis and insights about the edits were made [in this
notebook](https://github.com/leportella/wiki-analysis/blob/master/notebooks/insights/01-Insights-Edits.ipynb).

There were 22,126,031 records on the `edits.tsv` file. Near 9% of these edits
were reverted. The dataset started in May 2001 and ended in Aug 2010. 

Most of the edits were made in 2008 and 2009. Although 2010 had a smaller
number, we must consider that the records stopped in August. So, only we have
records of 66% of the year but, nevertheless, the number of edits until August
represents 74% of the edits recorded in 2009. Thus, we can consider that the
number of edits per year had an increase from 2001 to 2010. 

![](https://i.imgur.com/Zel9kcN.png)

It is possible to see that from 2002 to 2006 there was a huge increase in the
number of new edits per year. After 2006 the growth was smaller, but the edits
kept increasing overall.

![](https://i.imgur.com/uwrAHzu.png).

We can observe that the months from March until August have the higher number
of editing, while from September until December are slightly smaller. This could
be result of holidays. 

![](https://i.imgur.com/S1pK74Z.png)

Although no pattern appeared during the days of the month a good pattern
emerged from the data of hours of day. Usually, users tend to contribute on
afternoons and nights. Morning periods have a lot fewer edits, specially with
the minimum being between 7 and 10 am. 

![](https://i.imgur.com/OHQodon.png)

The users have an average of 134 edits reverted, with a standard deviation of
1063.9 edits. The max reverts per user was 59323, by user `416473`. Although
this is a very high number, this user made a total contribution of 146134. This 
represents only 40% of the user total contribution, so even if it very high, it's a 
number reasonable considering all users, and can't be considered an outlier.

The mean percent of edits reverted is 9%, but it can normally reach up to 50%.
Users that reached 100% of reverts, usually did a single edit that was
reverted.

![](https://i.imgur.com/dy2g2kT.png)


### Comments

The dataset `comments.tsv` had a total of 18,978,698 comments (18 million). A sample 
of 100,000 comments were obtained and evaluated quickly. Although the analysis is not 
very detailed, it is possible to see that the word `revert` is the 8th word more used 
in the comments analyzed and the word `add` was the third. This could be used as an 
additional feature for better predicting results in future implementations. 

![](https://i.imgur.com/DT6ZPru.png)


<h2 id='engineering'>Feature Engineering</h2>

<h3 id='engineering-first-revert'>Features used in First Revert Scenario</h3>

All feature engineering were made using [this notebook](https://github.com/leportella/wiki-analysis/blob/master/notebooks/provision/00-Edits-Feature-Engineering.ipynb).

The initial dataset used was a timeseries of edits of Wikipedia articles made 
by users. Since we wanted to predict the user behavior, the dataset needed to be 
worked with. Thus, all dataset used on the prediction was engineered based on the 
original `edits.tsv` dataset.

We needed to separate information of each user and only of users that had at least one 
article reverted, since we wanted to predict whether the user would edit again after the first 
revert. 

The original dataset had 22,126,031 records of article edition (22 million) with a total 
of 44,514 unique users. The final dataset had only 14,829 unique users, since not all users 
had reverts.

From this timeseries records of the original dataset, we engineered 9 features.
Each feature is described below:

| Feature                    | Detail                                                                  | Type     | Used on model |
|----------------------------|-------------------------------------------------------------------------|----------|---------------|
| Time of first revert       | Time when the first edit reverted occured                               | Datetime | No            |
| Time of first contribution | Time when the user made the first contribution                          | Datetime | No            |
| Time of last contribution  | Time when the user made the last contribution                           | Datetime | No            |
| Days until revert          | Number of days it took the user to have the first revert                | Integer  | Yes           |
| Days of contribution       | Number of days the user contributed                                     | Integer  | No            |
| Total updates              | Number of edits the user made                                           | Integer  | No            |
| Updated after revert       | If the user edited again after the revert (target)                      | Bool     | Yes           |
| Updates per day            | Mean number of edits made per day during the total time of contribution | Float    | Yes           |
| Edits before revert        | The number of edits the user made before having it's first revert       | Integer  | Yes           |


<h3 id='engineering-any-revert'>Features used in Any Revert Scenario</h3>

All feature engineering were made using [this notebook](https://github.com/leportella/wiki-analysis/blob/master/notebooks/provision/01-Edits-Feature-Engineering-Any-Revert.ipynb).

The initial dataset used was a timeseries of edits of Wikipedia articles made 
by users. Since we wanted to predict the behavior after each revert, the dataset needed to be 
worked with. Thus, all dataset used on the prediction was engineered based on the 
original `edits.tsv` dataset.

We needed to separate information of all edits that were reverted. 

The original dataset had 22,126,031 records of article edition (22 million). The dataset containing all reverted edits contained more than a million records. A sample of 500,000 records were gathered, to make the analysis more fast and the data, easier to work with. 

From this timeseries records of the original dataset, we engineered 10 features.
Each feature is described below:

| Feature                           | Detail                                                                                      | Type     | Used on model |
|-----------------------------------|---------------------------------------------------------------------------------------------|----------|---------------|
| Time of first contribution        | Time when the user made the first contribution                                              | Datetime | No            |
| Number of articles before revert  | Number of articles the user already made before this particular revert                      | Integer  | Yes           |
| Number of reverts before revert   | Number of reverts the user already had before this particular revert                        | Integer  | Yes           |
| Days before revert                | Number of days it took the user between first contribution an this particular revert revert | Integer  | Yes           |
| Time of last edit before revert   | Time the last edit before this particular revert occured                                    | Datetime | No            |
| Time of last revert before revert | Time the last revert before this particular revert occured                                  | Datetime | No            |
| Days after last edit              | Number of days this particular revert occured after last edit                               | Integer  | Yes           |
| Days after last revert            | Number of days this particular revert occured after last revert                             | Integer  | Yes           |
| Number of edits after revert      | Number of edits the user made after this particular revert                                  | Integer  | No            |
| Target                            | If the user edited again after this particular revert                                       | Bool     | Yes           |


<h2 id='first-revert-model'>Predicting if user will edit again after first revert</h2>

The notebooks with the attempts of getting the best model are available [here](https://github.com/leportella/wiki-analysis/tree/master/notebooks/modelling).

The initial strategy was to construct a "quick and dirt" model, with minimal features and 
a simple model, to understand how far from our goal we were. The notebook is 
available [here](https://github.com/leportella/wiki-analysis/blob/master/notebooks/modelling/01-Baseline_LogisticRegression.ipynb).

The baseline model (notebook 1) presented a high accuracy (0.95) but with no classification of 
class 0 (users that didn't update). Thus, the high accuracy was mainly due to a problem of unbalanced classes. 

The [second notebook](https://github.com/leportella/wiki-analysis/blob/master/notebooks/modelling/02-LogisticRegression-undersampling.ipynb) tried a strategy of undersampling. We took all 
records of class 0 (756) and the same amount of class 1, and end up with a total of 1512 records.This strategy resulted in an accuracy score of 0.73, but with high values of precision and recall for both class 0 and 1. 

On the [third notebook](https://github.com/leportella/wiki-analysis/blob/master/notebooks/modelling/03-RandomForest.ipynb), we used a Random Forest Classifier model, adding values of unbalanced 
classes as a parameter of the model. This is, we let the model know that our class 0 had just a few records, while class 1 prevailed with most of records. The precision of class 0 improved, but the recall remained too low for this model to be considered good enough.

On [notebook 4](https://github.com/leportella/wiki-analysis/blob/master/notebooks/modelling/04-RandomForest-undersampling.ipynb) the undersampling strategy was used along with the Random Forest model. The main accuracy was of 0.81 will all precision and recall values of both classes with values above 0.8. This was considered the best model.

Notebooks [5](https://github.com/leportella/wiki-analysis/blob/master/notebooks/modelling/05-GradientBoosting.ipynb) and [6](https://github.com/leportella/wiki-analysis/blob/master/notebooks/modelling/06-GradientBoosting-undersampling.ipynb) tried both strategies with Gradient Boosting. However, these models were not considered as good as the model achieved on Notebook 4. 

One observation must be made: the best hyperparameters for each models were obtained by the 
use of [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), which a technique specialized on finding the best hyperparameters based on a measure of accuracy.


### Final Model

The model had as input 1512 users that had at least one edit reverted. The dataset was divided 
into two parts: the **train** dataset and the **test** dataset, which represented 20% of the 
1512 users.

We used the classifier Random Forest, from the Scikit-learn, the GridSearchCV for finding the 
best hyperparameters and the accuracy score, as the measure of efficiency. The model was 
trained using the **train** dataset (80% of the data). Once the model was trained, the features 
of the **test** dataset was used to get the prediction based on this trained model, and the 
results were compared with the target variable of this **test** dataset. 

We achieved a model with a accuracy of 0.82, which was considered a good prediction. 
The model used three features only: the number of days 
before the first revert, the number of updated per day and the number of edits before the first 
revert. The model showed that the number of updates per day is the most important feature.

The results of this model can be seen below:

![](https://i.imgur.com/UNsuDJn.png)

![](https://i.imgur.com/ubxmL5X.png)


<h2 id='any-revert-model'>Predicting if user will edit again after any revert</h2>


The notebooks with the attempts of getting the best model are available [here](https://github.com/leportella/wiki-analysis/tree/master/notebooks/modelling).

The same initial strategy to construct a "quick and dirt" model was applied here to understand how far from our goal we were. The notebook is 
available [here](https://github.com/leportella/wiki-analysis/blob/master/notebooks/modelling/any_revert/1.01-Baseline_LogisticRegression.ipynb).

The baseline model (notebook 1) presented a high accuracy (0.99) but with no classification of 
class 0 (users that didn't update after the revert). Thus, the high accuracy was mainly due to a problem of unbalanced classes. 

The [second notebook](https://github.com/leportella/wiki-analysis/blob/master/notebooks/modelling/any_revert/1.02-LogisticRegression_undersampling.ipynb) tried a strategy of undersampling. We took all 
records of class 0 (1091) and the same amount of class 1, and end up with a total of 2182 records. This strategy resulted in an accuracy score of 0.68, but with good values of precision and recall for both class 0 and 1 (>0.5). 

On the [third notebook](https://github.com/leportella/wiki-analysis/blob/master/notebooks/modelling/any_revert/1.03-RandomForest_undersampling_GridSearchCV.ipynb), we used a Random Forest Classifier model with the same undersampling technique used on the best models found. This model had an accuracy of 0.86

One observation must be made: the best hyperparameters for each models were obtained by the 
use of [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), which a technique specialized on finding the best hyperparameters based on a measure of accuracy.

## Final Model

The model had as input 2062 records of reverted edits and if the user edited again after it, or not. The dataset was divided 
into two parts: the **train** dataset and the **test** dataset, which represented 30% of the 
2062 users.

We used the classifier Random Forest, from the Scikit-learn, the GridSearchCV for finding the 
best hyperparameters and the accuracy score, as the measure of efficiency. The model was 
trained using the **train** dataset (70% of the data). Once the model was trained, the features 
of the **test** dataset was used to get the prediction based on this trained model, and the 
results were compared with the target variable of this **test** dataset. 

We achieved a model with a accuracy of 0.88, which was considered a good prediction. 
The results of this model can be seen below:

![](https://i.imgur.com/juhLKTR.png)

![](https://i.imgur.com/gbQfK7I.png)


The model used five features. Three of them had a similar importance on the model: the number of days before the revert, the number of articles written before the revert and the number of reverts before the revert considered.

![](https://i.imgur.com/o6sbo9o.png)
